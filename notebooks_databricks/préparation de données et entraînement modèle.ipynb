{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14267627-66ff-4ba1-9ec2-7b494026865b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load('/mnt/databricks/kafka/transactions')\n",
    "df = df.withColumnRenamed(\"E\", \"event_time\") \\\n",
    "       .withColumnRenamed(\"s\", \"symbol\") \\\n",
    "       .withColumnRenamed(\"t_trade\", \"trade_id\") \\\n",
    "       .withColumnRenamed(\"p\", \"price\") \\\n",
    "       .withColumnRenamed(\"q\", \"quantity\") \\\n",
    "       .withColumnRenamed(\"T\", \"trade_time\") \\\n",
    "       .withColumnRenamed(\"m\", \"buyer_is_maker\") \\\n",
    "       .withColumnRenamed(\"M_flag\", \"ignore_flag\")\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1d0093-a6cf-4054-9e90-bce09564936f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lag, unix_timestamp, to_timestamp, hour\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# price et quantity en float (était en string)\n",
    "df = df.withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "       .withColumn(\"quantity\", col(\"quantity\").cast(\"double\"))\n",
    "\n",
    "# Calculer le montant total\n",
    "df = df.withColumn(\"amount\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "# Convertir en timestamp Spark\n",
    "df = df.withColumn(\"trade_time_ts\", to_timestamp((col(\"trade_time\") / 1000).cast(\"long\")))\n",
    "\n",
    "# Extraire l'heure du trade\n",
    "df = df.withColumn(\"hour_of_day\", hour(col(\"trade_time_ts\")))\n",
    "\n",
    "# Différence de prix par rapport à la transaction précédente (souvent 0)\n",
    "window = Window.orderBy(\"trade_time\")\n",
    "df = df.withColumn(\"prev_price\", lag(\"price\").over(window))\n",
    "df = df.withColumn(\"price_diff\", col(\"price\") - col(\"prev_price\"))\n",
    "\n",
    "#transformer colonne booléenne en numérique 0/1\n",
    "df = df.withColumn(\"buyer_is_maker_num\", when(col(\"buyer_is_maker\") == True, 1).otherwise(0))\n",
    "\n",
    "#Toutes les colonnes à prendre pour le modèle\n",
    "features_cols = [\n",
    "    \"price\",\n",
    "    \"quantity\",\n",
    "    \"buyer_is_maker_num\",\n",
    "    \"amount\",\n",
    "    \"hour_of_day\",\n",
    "    \"prev_price\",\n",
    "    \"price_diff\"\n",
    "]\n",
    "\n",
    "# Créer un vecteur de features avec VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# créer un df de features\n",
    "df_features = assembler.transform(df).select(\"features\")\n",
    "\n",
    "# on affiche pour voir si ça a marché correctement\n",
    "df_features.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d6d23e-358c-49b9-9e52-bcf02f18b8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from synapse.ml.isolationforest import IsolationForest\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "results_path = \"/mnt/databricks/delta/anomaly_results\"\n",
    "model_path = \"/mnt/databricks/models/my_model\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Logging des hyperparamètres\n",
    "    mlflow.log_param(\"featuresCol\", \"features\")\n",
    "    mlflow.log_param(\"predictionCol\", \"anomaly_prediction\")\n",
    "    mlflow.log_param(\"scoreCol\", \"anomaly_score\")\n",
    "    mlflow.log_param(\"contamination\", 0.01)\n",
    "    mlflow.log_param(\"maxSamples\", 1.0)\n",
    "\n",
    "    # Création et entraînement du modèle Isolation Forest\n",
    "    isolation_forest = IsolationForest() \\\n",
    "        .setFeaturesCol(\"features\") \\\n",
    "        .setPredictionCol(\"anomaly_prediction\") \\\n",
    "        .setScoreCol(\"anomaly_score\") \\\n",
    "        .setContamination(0.01) \\\n",
    "        .setMaxSamples(1.0)\n",
    "\n",
    "    model = isolation_forest.fit(df_features)\n",
    "\n",
    "    # Application du modèle\n",
    "    result = model.transform(df_features)\n",
    "\n",
    "    result.write.format(\"delta\").mode(\"overwrite\").save(results_path)\n",
    "\n",
    "    # taux d’anomalie détecté\n",
    "    anomaly_ratio = result.filter(result.anomaly_prediction == 1).count() / result.count()\n",
    "    mlflow.log_metric(\"anomaly_ratio\", anomaly_ratio)\n",
    "\n",
    "    # Log du modele dans MLflow\n",
    "    mlflow.spark.log_model(model, \"isolation_forest_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ccaca0-4499-41fe-a9e7-2b3e9b4af8fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle\n",
    "model_path = \"/mnt/databricks/models/my_model\"\n",
    "\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b5f0c4-9da0-400b-908d-df36451915cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Affichage résultats\n",
    "\n",
    "df = spark.read.format(\"delta\").load(results_path)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "préparation de données et entraînement modèle",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
