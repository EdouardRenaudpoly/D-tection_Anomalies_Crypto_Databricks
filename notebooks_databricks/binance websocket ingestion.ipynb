{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207da4f4-73bd-4943-a7e1-edc7f0b5191b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "confluentClusterName = \"cluster_0\"\n",
    "confluentBootstrapServers = \"\"\n",
    "confluentTopicName = \"binance-trades\"\n",
    "confluentApiKey = \"\"\n",
    "confluentSecret = \"\"\n",
    "schemaRegistryUrl = \"\"\n",
    "confluentRegistryApiKey = \"\"\n",
    "confluentRegistrySecret = \"\"\n",
    "deltaTablePath = \"/mnt/databricks/predictions_output\"\n",
    "checkpointPath = '/mnt/databricks/checkpoints/my_model_streaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa07a77-bc4c-48f3-8737-77eb6f261bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import StringType\n",
    "binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\n",
    "streamTestDf = (\n",
    "  spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "  .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "  .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\n",
    "  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "  .option(\"subscribe\", confluentTopicName)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"failOnDataLoss\", \"false\")\n",
    "  .load()\n",
    "  .withColumn('key', fn.col(\"key\").cast(StringType()))\n",
    "  .withColumn('fixedValue', fn.expr(\"substring(value, 6, length(value)-5)\"))\n",
    "  .withColumn('valueSchemaId', binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\n",
    "  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key', 'valueSchemaId','fixedValue')\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import from_json, col, expr, concat, lit, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"E\", LongType()),\n",
    "    StructField(\"s\", StringType()),\n",
    "    StructField(\"t_trade\", LongType()),\n",
    "    StructField(\"p\", StringType()),\n",
    "    StructField(\"q\", StringType()),\n",
    "    StructField(\"T\", LongType()),\n",
    "    StructField(\"m\", BooleanType()),\n",
    "    StructField(\"M_flag\", BooleanType()),  # modifié ici\n",
    "])\n",
    "\n",
    "decoded_df = streamTestDf.selectExpr(\"CAST(fixedValue AS STRING) as json_str\")\n",
    "\n",
    "json_fixed_df = decoded_df.select(\n",
    "    regexp_replace(regexp_replace(\"json_str\", r'(\"M\":)', '\"M_flag\":'), r'(\"t\":)', '\"t_trade\":').alias(\"fixed_json_str\")\n",
    ")\n",
    "\n",
    "json_clean_df = json_fixed_df.select(\n",
    "    concat(lit(\"{\"), expr(\"substring(fixed_json_str, instr(fixed_json_str, '\\\"E\\\"'))\")).alias(\"cleaned_json\")\n",
    ")\n",
    "\n",
    "parsed_df = json_clean_df.select(\n",
    "    from_json(col(\"cleaned_json\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# 1. Lance un writeStream vers une table temporaire mémoire\n",
    "query = (\n",
    "    parsed_df.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"parsed_kafka\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "import time\n",
    "waiting_time = 200\n",
    "time.sleep(waiting_time)\n",
    "\n",
    "# 3. Lire les données de la mémoire (batch statique)\n",
    "batch_df = spark.sql(\"SELECT * FROM parsed_kafka\")\n",
    "\n",
    "# 4. Écrire vers la Delta Table\n",
    "batch_df.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
    "\n",
    "# 5. Arrêter le stream\n",
    "query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "binance websocket ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
